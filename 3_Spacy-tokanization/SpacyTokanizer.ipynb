{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9fee97",
   "metadata": {},
   "source": [
    "### Import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3580480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sapcy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7f46da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "samosa\n",
      "chat\n",
      "of\n",
      "karachi\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "1\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# create a language object\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# create a document\n",
    "doc = nlp(\"Dr. Strange loves samosa chat of karachi as it costs only 1$ per plate.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80635514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing individual tokens\n",
    "doc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcd288c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr. Strange loves samosa"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Accessing slices\n",
    "doc[0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "354ccab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Accessing token attributes\n",
    "doc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f3418d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "# Another Sentence with prefinx and suffic\n",
    "# to avoid the signle quote issue in jupyter notebook \\ is used\n",
    "\n",
    "# doc2 = nlp('\"Let\\'s go to N.Y.!\"')\n",
    "# Or use tripple quotes\n",
    "\n",
    "doc2 = nlp('''\"Let's go to N.Y.!\"''')\n",
    "\n",
    "for token in doc2:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "916ffc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "902ab27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82f32466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strange loves samosa chat'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get slice of tokens\n",
    "doc[1:5]\n",
    "\n",
    "# or use the text attribute\n",
    "doc[1:5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da12b972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "doc3 = nlp(\"Tony gave two $ to Steve.\")\n",
    "toekn0 = doc3[0]\n",
    "toekn0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eb2c07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc3[2]\n",
    "token2.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76dd3cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12af2fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokanizer of spacy can also detect like_num as it is a number.\n",
    "token2.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d8263b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can detect currency symbol as well.\n",
    "token3 = doc3[3]\n",
    "token3.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1a36b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> is_alpha: True , is_stop: False , is_punct: False , like_num: False , is_currency: False\n",
      "gave ==> is_alpha: True , is_stop: False , is_punct: False , like_num: False , is_currency: False\n",
      "two ==> is_alpha: True , is_stop: True , is_punct: False , like_num: True , is_currency: False\n",
      "$ ==> is_alpha: False , is_stop: False , is_punct: False , like_num: False , is_currency: True\n",
      "to ==> is_alpha: True , is_stop: True , is_punct: False , like_num: False , is_currency: False\n",
      "Steve ==> is_alpha: True , is_stop: False , is_punct: False , like_num: False , is_currency: False\n",
      ". ==> is_alpha: False , is_stop: False , is_punct: True , like_num: False , is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token, \"==>\",\"is_alpha:\", token.is_alpha, \", is_stop:\", token.is_stop, \", is_punct:\", token.is_punct, \", like_num:\", token.like_num, \", is_currency:\", token.is_currency)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d7f0a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LIttle_Angel high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tBirthday   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Abdullah   5 June, 1882    abdullah@kohli.com\\n',\n",
       " 'Ahmad\\t12 April, 2001  ahmad@sharapova.com\\n',\n",
       " 'Majid  24 June, 1998   majid@williams.com \\n',\n",
       " 'Fardan      1 May, 1997    fardan@root.com']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading students data from text file\n",
    "with open(\"students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1543abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LIttle_Angel high school, 8th grade students information\\n ==================================================\\n \\n Name\\tBirthday   \\temail\\n -----\\t------------\\t------\\n Abdullah   5 June, 1882    abdullah@kohli.com\\n Ahmad\\t12 April, 2001  ahmad@sharapova.com\\n Majid  24 June, 1998   majid@williams.com \\n Fardan      1 May, 1997    fardan@root.com'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61592c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abdullah@kohli.com',\n",
       " 'ahmad@sharapova.com',\n",
       " 'majid@williams.com',\n",
       " 'fardan@root.com']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551e7c8",
   "metadata": {},
   "source": [
    "### work with urdu language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de5497ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مجھے\n",
      "1,000\n",
      "روپے\n",
      "ملے\n",
      "،\n",
      "لیکن\n",
      "خرچ\n",
      "بھی\n",
      "اتنے\n",
      "ہی\n",
      "ہو\n",
      "گئے\n",
      "۔\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy model for urdu\n",
    "nlp = spacy.blank(\"ur\") \n",
    "\n",
    "# Create a document\n",
    "doc5 = nlp(\"مجھے 1,000 روپے ملے، لیکن خرچ بھی اتنے ہی ہو گئے۔\")\n",
    "for token in doc5:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a015a9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token مجھے currency False number False\n",
      "Token 1,000 currency False number True\n",
      "Token روپے currency False number False\n",
      "Token ملے currency False number False\n",
      "Token ، currency False number False\n",
      "Token لیکن currency False number False\n",
      "Token خرچ currency False number False\n",
      "Token بھی currency False number False\n",
      "Token اتنے currency False number False\n",
      "Token ہی currency False number False\n",
      "Token ہو currency False number False\n",
      "Token گئے currency False number False\n",
      "Token ۔ currency False number False\n"
     ]
    }
   ],
   "source": [
    "for token in doc5:\n",
    "    print(\"Token\", token, \"currency\", token.is_currency, \"number\", token.like_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbaaa3b",
   "metadata": {},
   "source": [
    "## Customizing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6eac21ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'burger', 'and', '1.5L', 'coke']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese burger and 1.5L coke\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "67f0721d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'burger', 'and', '1.5L', 'coke']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Customizing the tokenizer\n",
    "# to split gimme into gim and me\n",
    "from spacy.symbols import ORTH\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [{ORTH: \"gim\"}, {ORTH: \"me\"}])\n",
    "\n",
    "doc = nlp(\"gimme double cheese burger and 1.5L coke\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7a45ad",
   "metadata": {},
   "source": [
    "## Tokanization or Segmentation of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cadac1c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m doc = nlp(\u001b[33m\"\u001b[39m\u001b[33mDr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msents\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hajiq\\anaconda3\\envs\\TF_env\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:926\u001b[39m, in \u001b[36msents\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba3dc83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline\n",
    "\n",
    "# there is nothing in the pipeline as we have loaded a blank model\n",
    "# because there is an error abvoe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65667bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x20433759850>)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add a sentencizer to the pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62e79b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange loves pav bhaji of Karachi.\n",
      "Hulk loves chat of Islamabad\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of Karachi. Hulk loves chat of Islamabad\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d1c24",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567aaad",
   "metadata": {},
   "source": [
    "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
    "\n",
    "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
